# GPT-from-Scratch

An step-by-step derivation and implementation of the GPT architecture from scratch, following the original papers on GPT and the transformer model. This is mostly a personal exercise to deepen my understanding on self-attention, transformer, causal languaging modelling and unsupervised pretraining, but can also serve as a guide for anyone interested to derive the GPT architecture from first principle.

## Technologies Used

- Python
- PyTorch

## Usage

To get started, run the Jupyter notebook titled `derive-gpt-from-scratch.ipynb`.

## Acknowledgments

This project is inspired and based on the following resources:

- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT Guide by Andrej Karpathy](https://m.youtube.com/watch?v=kCc8FmEb1nY)

## License

This project is licensed under the MIT License. Please see the [LICENSE](LICENSE) file for more details.
