# GPT-from-Scratch
![multi-head attention](reading_medusa.jpg)
*Multi-head attention (created with DALL.E 3*)

A step-by-step derivation and implementation of the GPT architecture from scratch, following the original paper on GPT: [Improving Language Understanding by Generative Pre-Training (Radford et al. 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) and the transformer model: [Attention is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762). This is mostly a personal exercise to deepen my understanding on self-attention, transformer, causal languaging modelling and unsupervised pretraining, but can also serve as a guide for anyone interested to derive the GPT architecture from first principle.

## Dependencies

- PyTorch>=2.1.0

## Usage

The complete derivation walkthrough is on the Jupyter notebook `derive-gpt-from-scratch.ipynb`.

## Acknowledgments

This project references the following resources:

- [Improving Language Understanding by Generative Pre-Training (Radford et al. 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Attention is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762)
- [GPT Guide by Andrej Karpathy](https://m.youtube.com/watch?v=kCc8FmEb1nY)

## License

This project is licensed under the MIT License. Please see the [LICENSE](LICENSE) file for more details.
