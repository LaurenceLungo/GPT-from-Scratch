# GPT-from-Scratch

A step-by-step derivation and implementation of the GPT architecture from scratch, following the original papers on GPT and the transformer model. This is mostly a personal exercise to deepen my understanding on self-attention, transformer, causal languaging modelling and unsupervised pretraining, but can also serve as a guide for anyone interested to derive the GPT architecture from first principle.

## Technologies Used

- Python
- PyTorch

## Usage

The complete derivation walkthrough is on the Jupyter notebook `derive-gpt-from-scratch.ipynb`.

## Acknowledgments

This project references the following resources:

- [Improving Language Understanding by Generative Pre-Training (Radford et al. 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Attention is All You Need (vaswani et al. 2017)](https://arxiv.org/abs/1706.03762)
- [GPT Guide by Andrej Karpathy](https://m.youtube.com/watch?v=kCc8FmEb1nY)

## License

This project is licensed under the MIT License. Please see the [LICENSE](LICENSE) file for more details.
