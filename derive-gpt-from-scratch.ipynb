{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch>=2\" numpy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download TinyShakespeare Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# all the unique chars that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 56, 39, 52, 57, 44, 53, 56, 51, 43, 56, 57]\n",
      "transformers\n"
     ]
    }
   ],
   "source": [
    "# Implement a simple chaacter level tokenization schema. More sophisticated tokenizers include SentencePiece / tiktoken\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # str to int mapping\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # int to str mapping\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"transformers\"))\n",
    "print(decode(encode(\"transformers\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input content is tensor([18]), the targer is 47\n",
      "When input content is tensor([18, 47]), the targer is 56\n",
      "When input content is tensor([18, 47, 56]), the targer is 57\n",
      "When input content is tensor([18, 47, 56, 57]), the targer is 58\n",
      "When input content is tensor([18, 47, 56, 57, 58]), the targer is 1\n",
      "When input content is tensor([18, 47, 56, 57, 58,  1]), the targer is 15\n",
      "When input content is tensor([18, 47, 56, 57, 58,  1, 15]), the targer is 47\n",
      "When input content is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the targer is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input content is {context}, the targer is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is tensor([24]), target is 43\n",
      "when input is tensor([24, 43]), target is 58\n",
      "when input is tensor([24, 43, 58]), target is 5\n",
      "when input is tensor([24, 43, 58,  5]), target is 57\n",
      "when input is tensor([44]), target is 53\n",
      "when input is tensor([44, 53]), target is 56\n",
      "when input is tensor([44, 53, 56]), target is 1\n",
      "when input is tensor([44, 53, 56,  1]), target is 58\n",
      "when input is tensor([52]), target is 58\n",
      "when input is tensor([52, 58]), target is 1\n",
      "when input is tensor([52, 58,  1]), target is 58\n",
      "when input is tensor([52, 58,  1, 58]), target is 46\n",
      "when input is tensor([25]), target is 17\n",
      "when input is tensor([25, 17]), target is 27\n",
      "when input is tensor([25, 17, 27]), target is 10\n",
      "when input is tensor([25, 17, 27, 10]), target is 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(batch_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target =yb[b,t]\n",
    "        print(f\"when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # the whole model is simply a square lookup table.\n",
    "        # For each char (token) a in the whole char set, we maintain the probability of char b appearing after a.\n",
    "        # So the size is vocab_size x vocab_size\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) -> Batch=4(batch size), Time=8(block_size), Channel=65(vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits & targets into 2D to cater for F.corss_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        # generate max_new_tokens tokens iteratively by looking at only the last token each time\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, steps, batch_size, block_size, lr=1e-3):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    for steps in range(steps):\n",
    "        xb, yb = get_batch('train', batch_size, block_size)\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True) # clear grads from the previous step\n",
    "        loss.backward() # calculate grads for all params\n",
    "        optimizer.step() # update params\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train(m, 10000, batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulsee\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider this toy example batch:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the bigram model is not communicating / paying attention at the (n-1, n-2, ... , 1)th tokens when predicting the (n+1)th token from the nth. Majority of the context info is lost.\n",
    "\n",
    "We need to derive a mechanism for the model to attend to previous tokens when predicting the future token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive aggregation: averaging past tokens (weakest from of \"communication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: by naive for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B, T, C)) # x bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a/ torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'a={a}')\n",
    "print(f'b={b}')\n",
    "print(f'c={c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: by matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation of x[b,t] = mean_{i<=t} x[b,i] can be simplified & optimized by a row-normalized (each row sums to 1) lower triangular matric @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2.1: get weights by dividing by row sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "xbow2: tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "wei = torch.tril(torch.ones(T, T)) # weight - the row-normalized lower triangular matrix\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = wei @ x # (B (auto broadcasted by torch), T, T) @ (B, T, C) --> (B, T, C)\n",
    "print(f\"wei: {wei}\")\n",
    "print(f\"xbow2: {xbow2}\")\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2.2: get weights by softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by turning all zeros to -inf in a lower tri mat, then applying softmax to row, we can get the same weights\n",
    "- another advantage of softmax is that it ensures **non negative weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) # T by T lower tri mat\n",
    "wei = wei.masked_fill(wei==0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the averaging head module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingHead(nn.Module):\n",
    "    \"\"\" one head of naive aggregation \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        wei = torch.tril(torch.ones(T, T))\n",
    "        wei = F.softmax(wei.masked_fill(wei==0, float('-inf')), dim=1) # T by T lower tri mat\n",
    "        agg_x = wei @ x # (T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return agg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[0.5820, 0.1338, 0.7995],\n",
      "         [0.3071, 0.6526, 0.6105],\n",
      "         [0.1575, 0.6983, 0.7883]]])\n",
      "agg_x: tensor([[[0.5820, 0.1338, 0.7995],\n",
      "         [0.4446, 0.3932, 0.7050],\n",
      "         [0.3489, 0.4949, 0.7328]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(500)\n",
    "x = torch.rand(1, 3, 3)\n",
    "\n",
    "ah = AveragingHead()\n",
    "agg_x = ah(x)\n",
    "print(f'x: {x}')\n",
    "print(f'agg_x: {agg_x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding aggregation head to Bigram LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A few changes on BigramLanguageModelV2 from BigramLanguageModel\n",
    "1. add position_embedding_table (along T / block_size axis) to capture positional info\n",
    "2. parameterize n_embed in embedding tables to configure # dimensions of embedding vectors\n",
    "3. add AggregationHead to establish the weakest form of communication between upper context of text\n",
    "4. Now that we have implemented positional embedding, we cannot feed idx longer than block_size, else we will get index out of range when accessing the positional embedding table. idx is cropped to the last block_size block during generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithAveragingHead(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.head = AveragingHead()\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        agg_x = self.head(x) # (B, C, C)\n",
    "        logits = self.lm_head(agg_x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits & targets into 2D to cater for F.corss_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        # generate max_new_tokens tokens iteratively by looking at only the last token each time\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BigramLanguageModelWithAveragingHead(\n",
       "    (token_embedding_table): Embedding(65, 32)\n",
       "    (position_embedding_table): Embedding(8, 32)\n",
       "    (head): AveragingHead()\n",
       "    (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.compile(BigramLanguageModelWithAveragingHead(vocab_size, n_embed=32))\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6696462631225586\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "train(m1, 20000, batch_size, block_size, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the training result is worse than vanilla bigram, indicating averaging is a bad communication mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JGDLAE, URII NIOG::o\n",
      "\n",
      "UTHDL:Ko h itn wdehr, ousit\n",
      "martl hio.\n",
      "hehr aad, ssrt yoh i satherm atn eonnth\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m1.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4: self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head performing self-attention\n",
    "head_size = 16 # output size of the Linear layers\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing self-attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, n_embed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False) # (C, head_size)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei *= C**-0.5 # Scaled attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=2) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding self-attention to bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithSelfAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.self_attention_head = Head(n_embed, n_embed, block_size)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.self_attention_head(x) # single head self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits & targets into 2D to cater for F.corss_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        # generate max_new_tokens tokens iteratively by looking at only the last token each time\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BigramLanguageModelWithSelfAttentionHead(\n",
       "    (token_embedding_table): Embedding(65, 32)\n",
       "    (position_embedding_table): Embedding(8, 32)\n",
       "    (self_attention_head): Head(\n",
       "      (key): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (query): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=32, bias=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = torch.compile(BigramLanguageModelWithSelfAttentionHead(vocab_size, n_embed=32))\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4343011379241943\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "train(m2, 5000, batch_size, block_size, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loss is significantly lower than the previous 2 versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prn ose fo,\n",
      "Mird porne wofl f hat dr chevesars thito the nto,\n",
      "\n",
      "TIO RCISCEOEOTe.\n",
      "Therd th thery toughe m,\n",
      "S:\n",
      "Bake th my:\n",
      "U:\n",
      "Ad oucingenkfe we wiel icthuxjom\n",
      "Burethe sot-qyu sod ple Gd, sho hall.\n",
      "Withalle fo.\n",
      "\n",
      "wees nde quuu ld\n",
      "Th men ro tohe oansten\n",
      "I nd horure'rt, dh troussa,\n",
      "s. Wine.\n",
      "\n",
      "Avised whto ter\n",
      "He eloversivin idsusptr te I!\n",
      "Fimpgelok meas cecon. Whabsat, has by blornet fine batry iveamblamy O:\n",
      "Seatlke, ay bret hatout ithir thus ing we oruthape hen dooutanth, tharave wan,\n",
      "O NGLlo fuay degorse arpithatst cofnom, dis,\n",
      "TETEHOSIS:\n",
      "ENTO: cuyran cke mecolery thonowwarspre matt dee Gr\n",
      "Wharf wakedourgther ot, wike t\n",
      "Sabe dy Ller ary:\n",
      "FuciO Lowrens fr bepo dor fr.\n",
      "\n",
      "That ongatinof tht rckis youcee you:\n",
      "Ntan\n",
      "Bind.\n",
      "Theod\n",
      "Gul lry bus ms oy apall the melove whakis ar tearem Mank sent Yedyot. Why, oubve fathan hpiray:\n",
      "D We sth thile kned wove thily ce mard Ile the,\n",
      "LO, st\n",
      "Jlf the secth I cakl feat burcers lert gt oo, plathoupes whe dusampiey oorthe.\n",
      "\n",
      "F: ous wilerdo os doud wothal yea theango in\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m2.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing multi-headed self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, num_heads, n_embed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size) for _ in range(num_heads)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding multi-headed self-attention to bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithMultiHeadedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.self_attention_heads = MultiHead(num_heads, n_embed, n_embed//num_heads, block_size) # output dimension of MultiHead --> num_heads * _embed//num_heads = n_embed\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.self_attention_heads(x) # single head self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits & targets into 2D to cater for F.corss_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        # generate max_new_tokens tokens iteratively by looking at only the last token each time\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BigramLanguageModelWithMultiHeadedSelfAttention(\n",
       "    (token_embedding_table): Embedding(65, 32)\n",
       "    (position_embedding_table): Embedding(8, 32)\n",
       "    (self_attention_heads): MultiHead(\n",
       "      (heads): ModuleList(\n",
       "        (0-3): 4 x Head(\n",
       "          (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "          (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "          (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = torch.compile(BigramLanguageModelWithMultiHeadedSelfAttention(vocab_size, n_embed=32, num_heads=4))\n",
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.310880661010742\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "train(m3, 5000, batch_size, block_size, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PUERENY:\n",
      "I,\n",
      "ANRCERY:\n",
      "YOu for bestadr che my', cresto the stame heast hak.\n",
      "E ie.\n",
      "\n",
      "And lid ther' to wis my mas awe thom sarabud ongingenkfe yougheandoty xjom\n",
      "Burethe so ay kbe diple Geass othiss.\n",
      "Witht Meod whawkes nom quat nod so'se rove heroas ton\n",
      "I nou orkee'rt, du trousst,\n",
      "stt,\n",
      "Te tlavis, Vw to ter\n",
      "HAHeloversivin ids met. Ge I!\n",
      "ENFagenck meave, con. Whabsat, ess by blat et fine batry if?\n",
      "mblamy Orave toke, ay brete fackt yous mutus ifh weistuthn mind my yoourth, de Cave want westtlard ay degorse and thatss cofnom:\n",
      "Thand\n",
      "sow woth weat a cuy an cke my man yo hon whadsprepeatt dee Gut;\n",
      "Bof wall our, O yo tun R\n",
      "Cod\n",
      "labe dagh my 'sy mencie adwcens-\n",
      "Bincand or fromranto ongatinof thest pay youcen you: thaven hosre: onsss,\n",
      "Burme's do oy and galds youn'd whak ande teare.\n",
      "MBak sveperedyou. Cut:\n",
      "Wheve im.\n",
      "Handpir your ge sthissthe kind wove thiln cat lome lone wits cas lall tre se they de lis we bt cest lert gonoo, plath mudst-yourssampeey oortve.\n",
      "\n",
      "Fo dos will po os dond eoth mern, Vint go in\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m3.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining transformer components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward layers of transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed*4), # following \"Attention is all you need\" -> hidden layer size = 4 * input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed*4, n_embed),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The transformer block\n",
    "\n",
    "For each block, we self attend, then feed forward, interspersed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiHead(n_head, n_embed, head_size, block_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiHead(n_head, n_embed, head_size, block_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(x) # residual\n",
    "        x = x + self.feed_forward(x) # residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, n_embed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed) # linear transformation to project self-attention outputs back to the residual pathway\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiHead(n_head, n_embed, head_size, block_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm_1(x)) # applies layer norm BEFORE attention (studies suggest this is better than the original architecture)\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x)) # applies layer norm BEFORE ffwd (studies suggest this is better than the original architecture\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed*4), # following \"Attention is all you need\" -> hidden layer size = 4 * input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed*4, n_embed),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout) # dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei *= C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=2)\n",
    "        wei = self.dropout(wei) # dropout\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed) # linear transformation to project self-attention outputs back to the residual pathway\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_head, n_embed, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiHead(n_head, n_embed, head_size, block_size, dropout)\n",
    "        self.feed_forward = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm_1(x)) # applies layer norm BEFORE attention (studies suggest this is better than the original architecture)\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x)) # applies layer norm BEFORE ffwd (studies suggest this is better than the original architecture\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the baby GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyGPT(nn.Module):\n",
    "\n",
    "    def __init__(self, block_size, n_embed, n_head, n_block, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.register_buffer('positional_intervals', torch.arange(block_size, device=self.device))\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_head, n_embed, block_size, dropout) for _ in range(n_block)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(self.positional_intervals[:T]) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits & targets into 2D to cater for F.corss_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        # generate max_new_tokens tokens iteratively by looking at only the last token each time\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, steps, batch_size, block_size, lr, eval_interval, eval_iters, device):\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split,  batch_size, block_size)\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    for iter in range(steps):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', batch_size, block_size)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.323585 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 80\n",
    "n_head = 4\n",
    "n_block = 4\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "babyGPT = BabyGPT(block_size, n_embed, n_head, n_block, dropout, device)\n",
    "babyGPT.to(device)\n",
    "print(sum(p.numel() for p in babyGPT.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3637, val loss 4.3542\n",
      "step 100: train loss 2.7283, val loss 2.7425\n",
      "step 200: train loss 2.5845, val loss 2.5770\n",
      "step 300: train loss 2.5027, val loss 2.4930\n",
      "step 400: train loss 2.4475, val loss 2.4404\n",
      "step 500: train loss 2.3895, val loss 2.3872\n",
      "step 600: train loss 2.3493, val loss 2.3460\n",
      "step 700: train loss 2.3156, val loss 2.3149\n",
      "step 800: train loss 2.2919, val loss 2.2969\n",
      "step 900: train loss 2.2826, val loss 2.2959\n",
      "step 1000: train loss 2.2504, val loss 2.2677\n",
      "step 1100: train loss 2.2379, val loss 2.2401\n",
      "step 1200: train loss 2.2160, val loss 2.2228\n",
      "step 1300: train loss 2.1993, val loss 2.2204\n",
      "step 1400: train loss 2.1933, val loss 2.2122\n",
      "step 1500: train loss 2.1610, val loss 2.1899\n",
      "step 1600: train loss 2.1473, val loss 2.1744\n",
      "step 1700: train loss 2.1388, val loss 2.1651\n",
      "step 1800: train loss 2.1276, val loss 2.1348\n",
      "step 1900: train loss 2.1288, val loss 2.1433\n",
      "step 2000: train loss 2.1180, val loss 2.1542\n",
      "step 2100: train loss 2.0918, val loss 2.1310\n",
      "step 2200: train loss 2.0934, val loss 2.1248\n",
      "step 2300: train loss 2.0730, val loss 2.1015\n",
      "step 2400: train loss 2.0698, val loss 2.1106\n",
      "step 2500: train loss 2.0516, val loss 2.0962\n",
      "step 2600: train loss 2.0451, val loss 2.0792\n",
      "step 2700: train loss 2.0382, val loss 2.0789\n",
      "step 2800: train loss 2.0310, val loss 2.0737\n",
      "step 2900: train loss 2.0255, val loss 2.0789\n",
      "step 3000: train loss 2.0171, val loss 2.0694\n",
      "step 3100: train loss 2.0117, val loss 2.0588\n",
      "step 3200: train loss 2.0053, val loss 2.0588\n",
      "step 3300: train loss 1.9999, val loss 2.0507\n",
      "step 3400: train loss 1.9875, val loss 2.0446\n",
      "step 3500: train loss 1.9761, val loss 2.0451\n",
      "step 3600: train loss 1.9912, val loss 2.0537\n",
      "step 3700: train loss 1.9699, val loss 2.0292\n",
      "step 3800: train loss 1.9603, val loss 2.0287\n",
      "step 3900: train loss 1.9634, val loss 2.0259\n",
      "step 4000: train loss 1.9511, val loss 2.0224\n",
      "step 4100: train loss 1.9570, val loss 2.0210\n",
      "step 4200: train loss 1.9467, val loss 2.0122\n",
      "step 4300: train loss 1.9357, val loss 2.0086\n",
      "step 4400: train loss 1.9308, val loss 2.0041\n",
      "step 4500: train loss 1.9350, val loss 2.0011\n",
      "step 4600: train loss 1.9317, val loss 1.9950\n",
      "step 4700: train loss 1.9168, val loss 1.9932\n",
      "step 4800: train loss 1.9123, val loss 1.9826\n",
      "step 4900: train loss 1.9013, val loss 1.9951\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "train(babyGPT, max_iters, batch_size, block_size, learning_rate, eval_interval, eval_iters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DioNows.\n",
      "\n",
      "LAKENRY II:\n",
      "Evinow, I your bey'Ustain, know.\n",
      "\n",
      "LANEENRY but MUGIZAM: theight not his Me\n",
      "Theyperch, bve and dichich bis mot:\n",
      "be horspothuss eved is,\n",
      "Iven heas ve I surss to have math on wited halg\n",
      "Whove prines Was Le willongs! As musion the frour pod,\n",
      "All yousesty the whe lech hume have a wrer, in Edo. GuAnd efor nown! You dom fatcUse parris dhy un.\n",
      "\n",
      "VONG:\n",
      "I'll youse my for wopbehoul dayery, and, your\n",
      "weim\n",
      "Gosplunged whend whither sway,\n",
      "Abe him band. Awsty do.\n",
      "\n",
      "HALOUCIUS:\n",
      "Dell'Ead the le'd amCan mernged, the pastll cangd for I they they ly broy cult.\n",
      "\n",
      "PORTHAMCLZE:\n",
      "NovAtiny thussunt a to man:\n",
      "Shall bsen boke, ging the grain?\n",
      "\n",
      "They:\n",
      "MARTIO:\n",
      "For to a maje a gahter he shosse cin and mrshese thy kis mend Pay atnin,\n",
      "I fors my lowe phath then\n",
      "Yould: I than mif.\n",
      "\n",
      "LRY ORKINING, HINGy:\n",
      "Houch's all me how for, hols hereht\n",
      "Ching the shalveny Soul foreath him, featere\n",
      "Whong, are as ll a thengautoddeter a-\n",
      "Sto shall Xh, waitis bal'd, theave dief is\n",
      "How as have I be you striery and rusted ani\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(babyGPT.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
